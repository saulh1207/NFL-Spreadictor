{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "import numpy as np\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Get target URL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.pro-football-reference.com/years/{0}/{1}.htm\"\n",
    "passing_url = \"https://www.pro-football-reference.com/years/2023/passing.htm\"\n",
    "rushing_url = \"https://www.pro-football-reference.com/years/2023/rushing.htm\"\n",
    "receiving_url = \"https://www.pro-football-reference.com/years/2023/receiving.htm\"\n",
    "defense_url = \"https://www.pro-football-reference.com/years/2023/defense.htm\"\n",
    "kicking_url = \"https://www.pro-football-reference.com/years/2023/kicking.htm\"\n",
    "punting_url = \"https://www.pro-football-reference.com/years/2023/punting.htm\"\n",
    "returns_url = \"https://www.pro-football-reference.com/years/2023/returns.htm\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Step 3: Create player data webscraping code -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_parser(site,year,stat):\n",
    "    year = str(year)\n",
    "    response = requests.get(site.format(year,stat))\n",
    "    stat = stat\n",
    "    if response.status_code == 429:\n",
    "        print(\"Too many requests\")\n",
    "    elif response.status_code != 200 and response.status_code != 429:\n",
    "        print(\"Error connecting: \" + str(response.status_code))\n",
    "    else:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Locate the table by its ID\n",
    "        table = soup.find('table', id=stat)\n",
    "\n",
    "        # Extract headers\n",
    "        if stat == \"passing\" or stat == \"receiving\":\n",
    "            headers = [th.getText() for th in table.find_all('tr')[0].find_all('th')]\n",
    "                # Extract rows\n",
    "            rows = table.find_all('tr')[1:]  # Skip header row\n",
    "\n",
    "            # Extract data\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                data.append([col.getText() for col in cols])\n",
    "\n",
    "            # Create a DataFrame\n",
    "            df = pd.DataFrame(data, columns=headers[1:])  # Skip the first header as it's for the row labels\n",
    "        else:\n",
    "            headers = [th.getText() for th in table.find_all('tr')[1].find_all('th')]\n",
    "            # Extract rows\n",
    "            rows = table.find_all('tr')[2:]  # Skip header row\n",
    "\n",
    "            # Extract data\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                data.append([col.getText().replace('*','').replace('+','') for col in cols])\n",
    "\n",
    "            # Create a DataFrame\n",
    "            df = pd.DataFrame(data, columns=headers[1:])  # Skip the first header as it's for the row labels\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_w_delay(url, year, stat):\n",
    "    retry_delay = 1\n",
    "    time.sleep(retry_delay)\n",
    "    retry_delay += random.uniform(-2,2)\n",
    "    df = year_parser(url,year,stat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2017, 2018, 2019, 2020, 2021, 2022, 2023,2024]\n",
    "\n",
    "def grabber(allstats):\n",
    "    allstats = allstats\n",
    "    df_dict = {}\n",
    "    data_dict  = {}\n",
    "    count = 1\n",
    "    for x in range(len(years)):\n",
    "        for i in range(len(allstats)):\n",
    "            names = str(years[x])+allstats[i]\n",
    "            targe = (str(years[x]),allstats[i])\n",
    "            df_dict[names] =  year_parser(url, *targe)\n",
    "            print(str(count)+\". parsed \"+names)\n",
    "            count += 1\n",
    "            time.sleep(10)\n",
    "    for key, value in df_dict.items():\n",
    "        if value is None:\n",
    "            print(\"Could not update \"+key+\" or it would be overwritten\")\n",
    "        else:\n",
    "            data_dict[key] = value\n",
    "            print(\"Successful update of \"+key)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Step 4: Run Webscrape -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"rushing\", \"passing\", \"defense\", \"receiving\", \"kicking\", \"punting\", \"returns\"]\n",
    "#[\"rushing\", \"passing\", \"defense\", \"receiving\", \"kicking\", \"punting\", \"returns\"]\n",
    "data_dict = grabber(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_list = player_data[\"2024kicking\"].Tm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(team_list)):\n",
    "    team_list[x] = team_list[x].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\"bal\":\"rav\", \"hou\":\"htx\", \"lvr\":\"rai\", \"ari\":\"crd\", \"lac\":\"sdg\", \"lar\":\"ram\", \"ten\":\"oti\", \"ind\":\"clt\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_teamlist = [replacements.get(s,s) for s in team_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_list = updated_teamlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in player_data.items():\n",
    "    data_dict[key] == data_dict[key].mask(data_dict[key].eq('None')).dropna()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"C:/Users/Saul/OneDrive/Documents/Saul's Folder/Coding/Betting Project/Data\"\n",
    "with pd.ExcelWriter(direct + \"/Historical Player Data.xlsx\",date_format=\"YYYY/MM/DD\") as writer:\n",
    "    for key, value in player_data.items():\n",
    "        player_data[key].to_excel(writer, sheet_name=str(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"2024passing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df:\n",
    "    if x is None:\n",
    "        print(True)\n",
    "    else:\n",
    "        print(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Step 5: Create Team Roster Webscraping Code -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_rosters(team_abbr,year):\n",
    "    # URL of the roster page for the given team and year\n",
    "    url = f\"https://www.pro-football-reference.com/teams/{team_abbr}/{year}_roster.htm\"\n",
    "    \n",
    "    # Fetch the page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Function to extract and clean data from the table\n",
    "    def extract_table_data(table):\n",
    "        if not table:\n",
    "            return None\n",
    "        headers = [th.getText() for th in table.find_all('tr')[0].find_all('th')]\n",
    "        rows = table.find_all('tr')[1:]  # Skip header row\n",
    "        table_data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                # Clean player names by removing * and † symbols\n",
    "                player_name = cols[0].getText().replace('*', '').replace('†', '').strip()\n",
    "                \n",
    "                # Extract other columns (position, games played, etc.)\n",
    "                row_data = [player_name] + [col.getText() for col in cols[1:]]\n",
    "                \n",
    "                # Append the row data to the list\n",
    "                table_data.append(row_data)\n",
    "        return pd.DataFrame(table_data, columns=headers[1:])  # Exclude first header (row label)\n",
    "\n",
    "    # Function to find the \"roster\" table (either in regular HTML or in comments)\n",
    "    def find_table(soup, table_id):\n",
    "        # First try finding the table directly in the HTML\n",
    "        table = soup.find('table', id=table_id)\n",
    "        if table:\n",
    "            return table\n",
    "\n",
    "        # If not found, try searching inside HTML comments\n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, str) and '<table' in text)\n",
    "        for comment in comments:\n",
    "            comment_soup = BeautifulSoup(comment, 'html.parser')\n",
    "            table = comment_soup.find('table', id=table_id)\n",
    "            if table:\n",
    "                return table\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Find the \"roster\" table (either in regular HTML or in comments)\n",
    "    roster_table = find_table(soup, 'roster')\n",
    "\n",
    "    # Extract data from the roster table\n",
    "    if roster_table:\n",
    "        df_roster = extract_table_data(roster_table)\n",
    "        # Insert the year column\n",
    "        df_roster.insert(0, 'Year', year)\n",
    "        return df_roster\n",
    "    else:\n",
    "        print(f\"No roster table found for {team_abbr} in {year}.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rostgrabber(teams):\n",
    "    df_dict = {}\n",
    "    rost_dict  = {}\n",
    "    count = 1\n",
    "    for x in range(len(years)):\n",
    "        for i in range(len(teams)):\n",
    "            names = str(years[x])+teams[i]\n",
    "            targe = (teams[i],str(years[x]))\n",
    "            df_dict[names] =  scrape_rosters(*targe)\n",
    "            print(str(count)+\". parsed \"+names)\n",
    "            count += 1\n",
    "            time.sleep(10)\n",
    "    for key, value in df_dict.items():\n",
    "        if value is None:\n",
    "            print(\"Could not update \"+key+\" or it would be overwritten\")\n",
    "            continue\n",
    "        else:\n",
    "            rost_dict[key] = value\n",
    "            print(\"Successful update of \"+key)\n",
    "\n",
    "    return rost_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roster_data = rostgrabber(team_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roster_data[\"2024clt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in roster_data.items():\n",
    "    roster_data[key] = roster_data[key].drop(roster_data[key].index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(direct + \"/Historical Rosters.xlsx\",date_format=\"YYYY/MM/DD\") as writer:\n",
    "    for key, value in roster_data.items():\n",
    "        roster_data[key].to_excel(writer, sheet_name=str(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in Data (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Handle missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert categorical columns to numerical using one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=['Team', 'Pos'])\n",
    "\n",
    "    # Normalize numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numerical_cols] = (df[numerical_cols] - df[numerical_cols].mean()) / df[numerical_cols].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "sports_data_processed = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define and Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_and_train_model(df):\n",
    "    # Split data into features and target\n",
    "    X = df.drop('spread', axis=1)  # 'spread' is the target variable\n",
    "    y = df['spread']\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Loss: {loss}')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_and_train_model(sports_data_processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spread(model, new_data):\n",
    "    # Preprocess new data similarly to training data\n",
    "    new_data_processed = preprocess_data(new_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(new_data_processed)\n",
    "    return predictions\n",
    "\n",
    "# Example prediction\n",
    "new_data = pd.DataFrame([{'team_home': 'Team A', 'team_away': 'Team B', ...}])  # Fill with appropriate features\n",
    "spread_prediction = predict_spread(model, new_data)\n",
    "print(spread_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Adjust the data preprocessing steps based on the specific structure and requirements of your data.\n",
    "- Ensure the target variable (spread in this example) is correctly defined and available in your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
